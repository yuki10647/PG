{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e950885",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import mean_squared_log_error as msle\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cccdafa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df =  pd.read_csv('trainofbike.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564c3020",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://amalog.hateblo.jp/entry/hyper-parameter-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f3c2a2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "            ...\n",
       "            2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
       "           dtype='int64', name='datetime', length=10886)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#特徴量を追加しています\n",
    "df.loc[:, \"hour\"] = pd.DatetimeIndex(df[\"datetime\"]).hour\n",
    "df.loc[:, \"month\"] = pd.DatetimeIndex(df[\"datetime\"]).month\n",
    "pd.DatetimeIndex(df[\"datetime\"]).weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "749396dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for分で回すためにデータのサイズを取得しています\n",
    "indice = len(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2f23a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for文で１行ずつ代入しています\n",
    "for i in range(indice):\n",
    "    if i == 0:\n",
    "        continue\n",
    "    workingday_value = df.loc[i, \"workingday\"]\n",
    "    hour_value = df.loc[i, \"hour\"]\n",
    "    df_tmp = df.loc[:i - 1].groupby([\"workingday\", \"hour\"]).mean()\n",
    "    try:\n",
    "        df.loc[i, \"mean\"] = df_tmp.loc[(workingday_value, hour_value)][\"count\"]\n",
    "  #初めてworkingday=1になったタイミングでkeyerrorが出るので、それを無視させています\n",
    "    except KeyError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89725946",
   "metadata": {},
   "outputs": [],
   "source": [
    "#１日目とworkingdayが最初に切り替わるタイミングでNaNが出るのでdrop\n",
    "df_ = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "527d193b",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasize = np.arange(len(df_.index))\n",
    "ind_train, ind_test = train_test_split(datasize, test_size=1000, random_state=1)\n",
    "df_train = df_.iloc[ind_train].reset_index()\n",
    "df_test = df_.iloc[ind_test].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eafe4d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#そのままサーチ\n",
    "features = ['holiday',\n",
    "            'workingday',\n",
    "            'weather',\n",
    "            'temp',\n",
    "            'atemp',\n",
    "            'humidity',\n",
    "            'windspeed',\n",
    "            'mean',\n",
    "            'hour',\n",
    "            'month']\n",
    "X_train = df_train[features]\n",
    "y_train = df_train[\"count\"]\n",
    "X_test = df_test[features]\n",
    "y_test = df_test[\"count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca295bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#対数変換用サーチ\n",
    "X_train = df_train[features]\n",
    "y_train = df_train[\"count\"]\n",
    "X_test = df_test[features]\n",
    "y_test = df_test[\"count\"]\n",
    "y_train_log = np.log1p(y_train + 1)\n",
    "y_test_log = np.log1p(y_test + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c07189",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf02078",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbc923e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61184786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000589 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 518\n",
      "[LightGBM] [Info] Number of data points in the train set: 9838, number of used features: 10\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Start training from score 191.585485\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "optimal_parameters\n",
      " {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 1, 'n_estimators': 1000, 'reg_alpha': 0.1, 'reg_lambda': 10, 'subsample': 0.1}\n",
      "Wall time: 12min 55s\n"
     ]
    }
   ],
   "source": [
    "#対数変換をしていない\n",
    "%%time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import lightgbm as lgb\n",
    "#合計45万通りの組み合せすべて検証は多すぎる(1000とおり)(4500)(20000)(3000)(1500)\n",
    "cv_params ={'max_depth':[9,10,11],\n",
    "            'min_child_weight':[1,2],\n",
    "            'subsample':[0.1, 0.2, 0.3],\n",
    "            'colsample_bytree':[0.7, 0.8, 0.9],\n",
    "            'reg_alpha':[1e-2, 0.1, 1],\n",
    "            'n_estimators':[1000, 2000],       #earlystop\n",
    "            'reg_lambda':[1, 10, 100],\n",
    "            'learning_rate':[0.1]\n",
    "           }\n",
    "\n",
    "model = lgb.LGBMRegressor(silent=False,n_jobs=-1)\n",
    "model_grid = GridSearchCV(model, cv_params, cv=5, n_jobs=-1)\n",
    "model_grid.fit(X_train,\n",
    "                y_train,\n",
    "                early_stopping_rounds=50,\n",
    "                eval_set=[(X_test, y_test)],\n",
    "                eval_metric='rmsle',\n",
    "                verbose=0)\n",
    "print('optimal_parameters\\n', model_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a46529b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000308 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 518\n",
      "[LightGBM] [Info] Number of data points in the train set: 9838, number of used features: 10\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Start training from score 4.625336\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "optimal_parameters\n",
      " {'colsample_bytree': 0.6, 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 1, 'n_estimators': 1000, 'reg_alpha': 0.01, 'reg_lambda': 0.1, 'subsample': 0.5}\n",
      "Wall time: 23min 48s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n                          max_depth = 11,           \\n                          min_child_weight = 1,     \\n                          subsample = 0.7,           \\n                          colsample_bytree = 0.8,   \\n                          reg_alpha = 1,           \\n                          n_estimators = 1000,     \\n                          reg_lambda = 0.01,           \\n                          learning_rate = 0.1        \\n                            \\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#対数変換した\n",
    "%%time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import lightgbm as lgb\n",
    "#合計45万通りの組み合せすべて検証は多すぎる((6561)（4600）\n",
    "cv_params ={'max_depth':[10,11],\n",
    "            'min_child_weight':[1,2],\n",
    "            'subsample':[0.5, 0.6, 0.7, 0.8],\n",
    "            'colsample_bytree':[0.6, 0.7, 0.8, 0.9],\n",
    "            'reg_alpha':[0.01, 0.1, 1,10],\n",
    "            'n_estimators':[1000, 2000],       #earlystop\n",
    "            'reg_lambda':[0.01, 0.01, 0.1],\n",
    "           }\n",
    "\n",
    "model = lgb.LGBMRegressor(silent=False,n_jobs=-1)\n",
    "model_grid = GridSearchCV(model, cv_params, cv=5, n_jobs=-1, scoring='neg_mean_squared_error')\n",
    "model_grid.fit(X_train,\n",
    "                y_train_log,\n",
    "                early_stopping_rounds=50,\n",
    "                eval_set=[(X_test, y_test_log)],\n",
    "                eval_metric='rmse',\n",
    "                verbose=0)\n",
    "print('optimal_parameters\\n', model_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "90753a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001392 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 518\n",
      "[LightGBM] [Info] Number of data points in the train set: 9838, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4.625336\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3150263742246829"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = lgb.LGBMRegressor(silent = False,           \n",
    "                          n_jobs = -1,\n",
    "                          max_depth = 10,           \n",
    "                          min_child_weight = 1,     \n",
    "                          subsample = 0.5,           \n",
    "                          colsample_bytree = 0.6,   \n",
    "                          reg_alpha = 0.01,           \n",
    "                          n_estimators = 1000,     \n",
    "                          reg_lambda = 0.1,           \n",
    "                          learning_rate = 0.1        \n",
    "                          )\n",
    "model.fit(X_train, y_train_log)\n",
    "y_pred_test = model.predict(X_test)\n",
    "y_pred = np.exp(y_pred_test) - 1\n",
    "np.sqrt(msle(y_test, y_pred))\n",
    "\n",
    "\"\"\"\n",
    "                          max_depth = 11,           \n",
    "                          min_child_weight = 1,     \n",
    "                          subsample = 0.7,           \n",
    "                          colsample_bytree = 0.8,   \n",
    "                          reg_alpha = 1,           \n",
    "                          n_estimators = 1000,     \n",
    "                          reg_lambda = 0.01,           \n",
    "                          learning_rate = 0.1        \n",
    "                            \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0075e71b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3225297286901196"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = lgb.LGBMRegressor()\n",
    "model.fit(X_train, y_train_log)\n",
    "y_pred_test = model.predict(X_test)\n",
    "y_pred = np.exp(y_pred_test) - 1\n",
    "np.sqrt(msle(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e94f2bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000148 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 517\n",
      "[LightGBM] [Info] Number of data points in the train set: 8744, number of used features: 10\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Start training from score 191.858188\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000437 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 517\n",
      "[LightGBM] [Info] Number of data points in the train set: 8745, number of used features: 10\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Start training from score 191.543282\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000202 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 518\n",
      "[LightGBM] [Info] Number of data points in the train set: 8745, number of used features: 10\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Start training from score 191.574614\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000473 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 516\n",
      "[LightGBM] [Info] Number of data points in the train set: 8745, number of used features: 10\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Start training from score 191.821841\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000455 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 517\n",
      "[LightGBM] [Info] Number of data points in the train set: 8745, number of used features: 10\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Start training from score 191.299714\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000496 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 517\n",
      "[LightGBM] [Info] Number of data points in the train set: 8745, number of used features: 10\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Start training from score 191.652487\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000479 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 517\n",
      "[LightGBM] [Info] Number of data points in the train set: 8745, number of used features: 10\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Start training from score 191.719611\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000163 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 516\n",
      "[LightGBM] [Info] Number of data points in the train set: 8745, number of used features: 10\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Start training from score 191.960320\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000440 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 514\n",
      "[LightGBM] [Info] Number of data points in the train set: 8745, number of used features: 10\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Start training from score 190.839337\n",
      "mean_score:0.3629395181681943\n",
      "\n",
      "plus=14542.695599077602\n",
      "equal=0.0\n",
      "minus=-14474.405695952802\n"
     ]
    }
   ],
   "source": [
    "#対数変換に対応していないものしかない\n",
    "#回帰する\n",
    "import lightgbm as lgb\n",
    "kf = KFold(n_splits=9, shuffle=True, random_state=1).split(X_train, y_train)\n",
    "kf_ = list(kf)\n",
    "model = lgb.LGBMRegressor(silent = False,           \n",
    "                          n_jobs = -1,\n",
    "                          max_depth = 10,           \n",
    "                          min_child_weight = 2,     \n",
    "                          subsample = 0.9,           \n",
    "                          colsample_bytree = 1.0,   \n",
    "                          reg_alpha = 100,           \n",
    "                          n_estimators = 1000,     \n",
    "                          reg_lambda = 0.1,           \n",
    "                          learning_rate = 0.1        \n",
    "                          )\n",
    "scores =[]\n",
    "score = 0\n",
    "\n",
    "diffs = []\n",
    "for train, valid in kf_:\n",
    "    model.fit(X_train.loc[train],\n",
    "              y_train.loc[train],\n",
    "              early_stopping_rounds=50,\n",
    "              eval_set=[(X_train.loc[valid], y_train.loc[valid])],\n",
    "              eval_metric='rmse',\n",
    "              verbose=0\n",
    "              )\n",
    "    prediction = model.predict(X_train.loc[valid])\n",
    "    prediction[prediction < 0] = 0\n",
    "    score = np.sqrt(msle(y_train.loc[valid], prediction))\n",
    "    scores.append(score)\n",
    "    y_true = y_train.loc[valid]\n",
    "    df_diff = prediction - y_true\n",
    "    plus = df_diff[df_diff > 0].sum()\n",
    "    equal = df_diff[df_diff == 0].sum()\n",
    "    minus = df_diff[df_diff < 0].sum()\n",
    "    _ = [plus, equal, minus]\n",
    "    diffs.append(_)\n",
    "mean_score = np.mean(scores) \n",
    "diff_mean = np.mean(np.array(diffs), axis=0)\n",
    "print(f\"mean_score:{mean_score}\\n\")\n",
    "print(f\"plus={diff_mean[0]}\")\n",
    "print(f\"equal={diff_mean[1]}\")\n",
    "print(f\"minus={diff_mean[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aaef8d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ab06be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tuning\n",
    "train0.39   test0.38\n",
    "#tuningなし\n",
    "train0.36   test0.34\n",
    "#default\n",
    "train0.36   test0.37\n",
    "#logとってないから、おそらくRSME計算になっているのかも"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1a4520",
   "metadata": {},
   "outputs": [],
   "source": [
    "#logとってやってみる\n",
    "#default\n",
    "test 0.32\n",
    "#tuning\n",
    "test 0.31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b68236",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d233d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10c04dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50aae642",
   "metadata": {},
   "outputs": [],
   "source": [
    "#対数変換した\n",
    "%%time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import lightgbm as lgb\n",
    "#合計45万通りの組み合せすべて検証は多すぎる((6561)（4600）\n",
    "cv_params ={'reg_alpha':[0.01, 0.1, 1,10],\n",
    "            'reg_lambda':[0.01, 0.01, 0.1],\n",
    "            'num_leaves':[2~131072],\n",
    "            'colsample_bytree':[0.6, 0.7, 0.8, 0.9],\n",
    "            'subsample':[0.5, 0.6, 0.7, 0.8],\n",
    "           }\n",
    "\n",
    "model = lgb.LGBMRegressor(silent=False,n_jobs=-1)\n",
    "model_grid = GridSearchCV(model, cv_params, cv=5, n_jobs=-1, scoring='neg_mean_squared_error')\n",
    "model_grid.fit(X_train,\n",
    "                y_train_log,\n",
    "                early_stopping_rounds=50,\n",
    "                eval_set=[(X_test, y_test_log)],\n",
    "                eval_metric='rmse',\n",
    "                verbose=0)\n",
    "print('optimal_parameters\\n', model_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b469bd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://qiita.com/c60evaporator/items/351188110f328ff921b9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a7a88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#没　そもそもRMSLEがなかった。\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import lightgbm as lgb\n",
    "#探索空間（パラメータ候補）を定義する。（合計45万通りの組み合せ）\n",
    "cv_params ={'max_depth':[10],\n",
    "            'min_child_weight':[1,2,3,4,5],\n",
    "            'subsample':[i/10.0 for i in range(6,11)],\n",
    "            'colsample_bytree':[0.7],\n",
    "            'reg_alpha':[1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 0.1, 1],\n",
    "            'n_estimators':[3000],\n",
    "            'reg_lambda':[1e-5, 1e-4, 1e-3, 1e-2, 0.1, 1],\n",
    "            'learning_rate':[0.1]\n",
    "            }\n",
    "model = lgb.LGBMRegressor(silent=False,n_jobs=-1)\n",
    "model_rand = RandomizedSearchCV(model, cv_params, n_iter=200, cv=5, n_jobs=-1)\n",
    "model_rand.fit(X_train,\n",
    "               y_train,\n",
    "               early_stopping_rounds=50,\n",
    "               eval_set=[(X_test, y_test)],\n",
    "               eval_metric='rmse',\n",
    "               verbose=0\n",
    "              )\n",
    "print('optimal_parameters\\n', model_rand.best_params_)\n",
    "\"\"\"\n",
    "optimal_parameters\n",
    " {'subsample': 0.6, 'reg_lambda': 0.0001, 'reg_alpha': 1e-05, 'n_estimators': 2000, 'min_child_weight': 3, 'max_depth': 8, 'learning_rate': 0.1, 'colsample_bytree': 0.7}\n",
    "    \n",
    " {'subsample': 1.0, 'reg_lambda': 1e-05, 'reg_alpha': 0.0001, 'n_estimators': 2000, 'min_child_weight': 5, 'max_depth': 7, 'learning_rate': 0.1, 'colsample_bytree': 0.7}\n",
    "\n",
    " {'subsample': 0.9, 'reg_lambda': 1e-05, 'reg_alpha': 1e-05, 'n_estimators': 2000, 'min_child_weight': 1, 'max_depth': 7, 'learning_rate': 0.1, 'colsample_bytree': 0.8}\n",
    "    \n",
    " {'subsample': 0.9, 'reg_lambda': 1, 'reg_alpha': 1, 'n_estimators': 2000, 'min_child_weight': 2, 'max_depth': 8, 'learning_rate': 0.1, 'colsample_bytree': 0.7}\n",
    "\n",
    " {'subsample': 0.7, 'reg_lambda': 0.001, 'reg_alpha': 0.001, 'n_estimators': 2000, 'min_child_weight': 3, 'max_depth': 7, 'learning_rate': 0.1, 'colsample_bytree': 0.8}\n",
    "\n",
    "調整\n",
    "\n",
    "{'subsample': 0.6, 'reg_lambda': 0.001, 'reg_alpha': 1e-06, 'n_estimators': 2000, 'min_child_weight': 1, 'max_depth': 10, 'learning_rate': 0.1, 'colsample_bytree': 0.7}\n",
    "\n",
    " {'subsample': 1.0, 'reg_lambda': 0.01, 'reg_alpha': 0.01, 'n_estimators': 3000, 'min_child_weight': 3, 'max_depth': 10, 'learning_rate': 0.1, 'colsample_bytree': 0.7}\n",
    " \n",
    "  {'subsample': 1.0, 'reg_lambda': 0.01, 'reg_alpha': 0.01, 'n_estimators': 3000, 'min_child_weight': 1, 'max_depth': 10, 'learning_rate': 0.1, 'colsample_bytree': 0.7}\n",
    "  \n",
    "   {'subsample': 0.7, 'reg_lambda': 0.01, 'reg_alpha': 1e-06, 'n_estimators': 3000, 'min_child_weight': 1, 'max_depth': 10, 'learning_rate': 0.1, 'colsample_bytree': 0.7}\n",
    "\n",
    "{'subsample': 0.9, 'reg_lambda': 0.001, 'reg_alpha': 1e-05, 'n_estimators': 2000, 'min_child_weight': 3, 'max_depth': 10, 'learning_rate': 0.1, 'colsample_bytree': 0.7}\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
